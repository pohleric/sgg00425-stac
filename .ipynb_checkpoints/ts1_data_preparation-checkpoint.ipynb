{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS-1: Data preparation\n",
    "\n",
    "*****\n",
    "\n",
    "This notebook allows you to load and pre-process an SDC dataset, which you can then save into a NetCDF (.nc) file to be reused quickly in other Notebooks where you do your analysis.\n",
    "\n",
    "Things you should change:\n",
    "\n",
    "* The config_cell variables\n",
    "* The output filename of the netcdf file (see the last cell).\n",
    "\n",
    "Then, note that the Notebook has two different options depending on the dataset that you want to pre-process:\n",
    "\n",
    "* Landsat\n",
    "* Land use statistics\n",
    "\n",
    "Only execute the section which corresponds to the product that you specified in the config_cell!\n",
    "\n",
    "*****\n",
    "<span style=\"color:red\">\n",
    "    \n",
    "Key processing steps in this notebook:\n",
    "- Definition of *bad* pixels (clouds, fill values, ...)\n",
    "- Setting bad pixels to `NA`\n",
    "- Scaling with scaling factors (original values are stored as `Integer` but need to be transformed to `Float`)\n",
    "- Cleaning empty timesteps (scenes with no valid data)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# reload module before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from odc.stac import stac_load\n",
    "import time\n",
    "import psutil\n",
    "import dask.distributed\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pystac_client import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sdc_utilities import *\n",
    "# silence warning (not recommended during development)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ds_clean = None\n",
    "ds_astat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the dataset configuration information:\n",
    "- product\n",
    "- geographical extent\n",
    "- time period\n",
    "- bands\n",
    "- ...\n",
    "\n",
    "You can generate it in three ways:\n",
    "1. manually from scratch,\n",
    "2. by manually copy/pasting the final cell content of the [config_tool](config_tool.ipynb) notebook,\n",
    "3. by loading the final cell content of the [config_tool](config_tool.ipynb) notebook using the magic `%load config_cell.txt`. (You need to execute the cell twice - 1. loading & 2. execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"config_cell.txt\"\n",
    "# Configuration\n",
    "\n",
    "product = 'landsat_ot_c2_l2'\n",
    "measurements = ['QA_PIXEL', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'ST_B10']\n",
    "aliases = ['QA_PIXEL', 'blue', 'green', 'red', 'nir', 'swir_1', 'swir_2', 'surface_temperature']  # you can also provide only the aliases and get the measurements with:\n",
    "# measurements, aliases = get_alias_band(aliases)\n",
    "# to make your live easier you can manually replace the measurements variable by \n",
    "# one of their alias:\n",
    "\n",
    "longitude = (7.127, 7.199)\n",
    "latitude = (46.773, 46.816)\n",
    "crs = 'epsg:4326'\n",
    "\n",
    "time = ('2016-04-01', '2016-07-01')\n",
    "# the following date formats are also valid:\n",
    "# time = ('2000-01-01', '2001-12-31')\n",
    "# time=('2000-01', '2001-12')\n",
    "# time=('2000', '2001')\n",
    "\n",
    "# You can use an UTM zone according to the DataCube System.\n",
    "# We prefer not to use this, instead specifying SwissGrid (epsg:2056).\n",
    "# output_crs = 'epsg:2056'\n",
    "\n",
    "output_crs = 'epsg:2056'\n",
    "resolution = -30.0, 30.0\n",
    "\n",
    "# These are the pixel classifications for Sentinel (SCL) and Landsat (QA_PIXEL); \n",
    "# you can use values to mask out values that belong to certain classes\n",
    "\n",
    "###################################\n",
    "# SCL categories:                 #\n",
    "#   0 - no data                   #\n",
    "#   1 - saturated or defective    #\n",
    "#   2 - dark area pixels          #\n",
    "#   3 - cloud_shadows             #\n",
    "#   4 * vegetation                #\n",
    "#   5 * not vegetated             #\n",
    "#   6 * water                     #\n",
    "#   7 * unclassified              #\n",
    "#   8 - cloud medium probability  #\n",
    "#   9 - cloud high probability    #\n",
    "#  10 - thin cirrus               #\n",
    "#  11 * snow                      #\n",
    "###################################\n",
    "\n",
    "# Check for more detailed information: \n",
    "# - Landsat 8/9 (OLI/TIRS), Page 19:\n",
    "# https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/media/files/LSDS-1619_Landsat8-9-Collection2-Level2-Science-Product-Guide-v6.pdf\n",
    "# - Landsat 7 (ETM+), Page 15:\n",
    "# https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/media/files/LSDS-1337_Landsat7ETM-C2-L2-DFCB-v6.pdf\n",
    "# - Landsat 4,5 (TM), Page 18:\n",
    "# https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1415_Landsat4-5-TM-C2-L1-DFCB-v3.pdf\n",
    "\n",
    "#############################################\n",
    "# QA_PIXEL BITS : CATEGORIES                #\n",
    "#    0 : Fill                               #\n",
    "#    1 : Clear                              #\n",
    "#    2 : Water                              #\n",
    "#    3 : Cloud shadow                       #\n",
    "#    4 : Snow                               #\n",
    "#    5 : Cloud                              #\n",
    "#   10 : Terrain occlusion (Landsat 8 only) #\n",
    "#############################################\n",
    "\n",
    "chunks = {\"x\": 2048, \"y\": 2048, \"time\": 1}  # 2048 values are OK with ~21Gb memory available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"config_cell.txt\"\n",
    "# Configuration\n",
    "\n",
    "product = 'landsat_ot_c2_l2'\n",
    "measurements = ['QA_PIXEL', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']\n",
    "aliases = ['QA_PIXEL', 'blue', 'green', 'red', 'nir', 'swir_1', 'swir_2']  # you can also provide only the aliases and get the measurements with:\n",
    "# measurements, aliases = get_alias_band(aliases)\n",
    "# to make your live easier you can manually replace the measurements variable by \n",
    "# one of their alias:\n",
    "\n",
    "longitude = (7.127, 7.199)\n",
    "latitude = (46.773, 46.816)\n",
    "crs = 'epsg:4326'\n",
    "\n",
    "time = ('2016-04-01', '2016-07-01')\n",
    "# the following date formats are also valid:\n",
    "# time = ('2000-01-01', '2001-12-31')\n",
    "# time=('2000-01', '2001-12')\n",
    "# time=('2000', '2001')\n",
    "\n",
    "# You can use an UTM zone according to the DataCube System.\n",
    "# We prefer not to use this, instead specifying SwissGrid (epsg:2056).\n",
    "# output_crs = 'epsg:2056'\n",
    "\n",
    "output_crs = 'epsg:2056'\n",
    "resolution = -30.0, 30.0\n",
    "\n",
    "# These are the pixel classifications for Sentinel (SCL) and Landsat (QA_PIXEL); \n",
    "# you can use values to mask out values that belong to certain classes\n",
    "\n",
    "###################################\n",
    "# SCL categories:                 #\n",
    "#   0 - no data                   #\n",
    "#   1 - saturated or defective    #\n",
    "#   2 - dark area pixels          #\n",
    "#   3 - cloud_shadows             #\n",
    "#   4 * vegetation                #\n",
    "#   5 * not vegetated             #\n",
    "#   6 * water                     #\n",
    "#   7 * unclassified              #\n",
    "#   8 - cloud medium probability  #\n",
    "#   9 - cloud high probability    #\n",
    "#  10 - thin cirrus               #\n",
    "#  11 * snow                      #\n",
    "###################################\n",
    "\n",
    "# Check for more detailed information: \n",
    "# - Landsat 8/9 (OLI/TIRS), Page 19:\n",
    "# https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/media/files/LSDS-1619_Landsat8-9-Collection2-Level2-Science-Product-Guide-v6.pdf\n",
    "# - Landsat 7 (ETM+), Page 15:\n",
    "# https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/media/files/LSDS-1337_Landsat7ETM-C2-L2-DFCB-v6.pdf\n",
    "# - Landsat 4,5 (TM), Page 18:\n",
    "# https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1415_Landsat4-5-TM-C2-L1-DFCB-v3.pdf\n",
    "\n",
    "#############################################\n",
    "# QA_PIXEL BITS : CATEGORIES                #\n",
    "#    0 : Fill                               #\n",
    "#    1 : Clear                              #\n",
    "#    2 : Water                              #\n",
    "#    3 : Cloud shadow                       #\n",
    "#    4 : Snow                               #\n",
    "#    5 : Cloud                              #\n",
    "#   10 : Terrain occlusion (Landsat 8 only) #\n",
    "#############################################\n",
    "\n",
    "chunks = {\"x\": 2048, \"y\": 2048, \"time\": 1}  # 2048 values are OK with ~21Gb memory available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you loaded the cell above from the config_cell.txt:\n",
    "# Did you run the cell again above after loading the config?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Landsat satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work around with dask\n",
    "client = dask.distributed.Client()\n",
    "catalog = Client.open(\"https://explorer.swissdatacube.org/stac\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with parameters defined in the config cell\n",
    "dataset_in = load_product_ts(catalog=catalog,\n",
    "                        product=product,\n",
    "                        longitude=longitude,\n",
    "                        latitude=latitude,\n",
    "                        output_crs=output_crs,\n",
    "                        measurements=measurements,\n",
    "                        resolution = resolution,\n",
    "                        time=time,\n",
    "                        chunks=chunks,\n",
    "                        rename=True,\n",
    "                        alias_names = aliases\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot first 5 scenes\n",
    "dataset_in.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot first 5 scenes\n",
    "dataset_in.blue[0:5,:,:].plot(col='time', vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_in.to_netcdf('dataset_in.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defined cloud identification and classification\n",
    "\n",
    "Check the notebook `help_cloudcover.ipynb` for more information.\n",
    "It makes sense to find out first what pixel classification (clouds, water, land, etc.) works best for your study region. Some classes might not work in mountains and on snow. Then a manual selection (plotting several scenes and identify the dates you want) might be your best choice.\n",
    "\n",
    "The overview of pixel classifications and quality flags can be found online. The links are provided in the `config_cell.txt` file, and screenshots of these tables in the folder `data/` with the names `Landsat8_QA_PIXEL.png`, `Landsat4-7_QA_PIXEL.png`, and `Sentinel2_SCL.png`.\n",
    "\n",
    "<img src=\"https://www.dropbox.com/scl/fi/zslaub479bjrpmqnlrkb3/Landsat8_QA_PIXEL.png?rlkey=jglvektr4mw7vax5bcm97jd1p&dl=1\" width=\"600\" />\n",
    "\n",
    "*Figure 1: Landsat 8 QA_PIXEL Bit flags example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We want to mask out the 'fill' values (no measurement) and 'cloud' \n",
    "bit_positions = [0,3]  \n",
    "ds_tmp = create_mask_from_bits(dataset_in, bit_positions)\n",
    "\n",
    "# # For Sentinel 2 see the \"help_cloudcover\" notebook\n",
    "# invalid_values = [0,1,9,10]  \n",
    "# ds_tmp = create_mask_from_values(ds_tmp, invalid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_tmp.to_netcdf('ds_tmp.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all values of identified pixels to not assigned 'np.nan'\n",
    "ds_na = ds_tmp.where(ds_tmp['mask'] != 1, other=np.nan)\n",
    "\n",
    "# # to drop all empty timesteps and the QA_PIXEL/SCL layer:\n",
    "# ds_clean = ds_na_reduced.dropna(dim='time', how='all')\n",
    "\n",
    "# to keep the QA_PIXEL/SCL layer, you need to select one band that is NOT the quality layer\n",
    "_band_check = 'blue'\n",
    "ds_clean = ds_na.dropna(dim='time', how='all', subset=[_band_check])\n",
    "\n",
    "# there are some issues with the CRS. This one-liner makes sure the CRS is working as intended.\n",
    "ds_clean = fix_crs(ds_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_clean.to_netcdf('ds_na_test.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 5 scenes:\n",
    "ds_clean.blue[0:5,:,:].plot(col='time', vmin=0, cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: add normalised difference index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CELL TO CALCULATE NDIs\n",
    "# You can already calculate normalised difference indexes here to be saved with the measurements.\n",
    "# To do this, use the relevant line(s) below and/or add your own.\n",
    "\n",
    "ds_clean['ndvi'] = (ds_clean.nir - ds_clean.red) / (ds_clean.nir + ds_clean.red)\n",
    "ds_clean['ndwi'] = (ds_clean.green - ds_clean.nir) / (ds_clean.green + ds_clean.nir)\n",
    "\n",
    "# 'NDWI': '(ds.green - ds.nir) / (ds.green + ds.nir)',\n",
    "# 'NDBI': '(ds.swir2 - ds.nir) / (ds.swir2 + ds.nir)'\n",
    "\n",
    "# As always, fix the CRS:\n",
    "ds_clean = fix_crs(ds_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at the summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean.ndvi[0:5,:,:].plot(col='time', vmin=0, cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 5 scenes:\n",
    "ds_clean.blue[0:5,:,:].plot(col='time', vmin=0, cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean.ndwi[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_clean.to_netcdf('after_ndvi.nc', engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add land use statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we manually change the variables `product` and `measurements` to specify what we want to load from arealstatistik.\n",
    "# We leave longitude, latitude, resolution, output_crs exactly as they were for Landsat. \n",
    "# This ensures that the data from arealstatistik will match the spatial coordinates of Landsat perfectly.\n",
    "\n",
    "# Specify the arealstatistik product\n",
    "product = ['arealstatistik']\n",
    "\n",
    "# Here, the measurements are not individual colour bands, \n",
    "# but instead are the different surveys with the desired number of classes.\n",
    "# By default we are loading the surveys for the most recent time period: 2013-2018.\n",
    "# To see all the available surveys, refer to the arealstatistik PDF document and explore_datacube.ipynb.\n",
    "measurements = ['AS18_4', 'AS18_17', 'AS18_27', 'AS18_72']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = catalog.search(\n",
    "    collections=[product],\n",
    "    limit=100,\n",
    "    bbox=(longitude[0], latitude[0],\n",
    "          longitude[1], latitude[1])\n",
    ")\n",
    "items = list(query.items())\n",
    "\n",
    "# load identified items\n",
    "ds_astat = stac_load(\n",
    "    items,\n",
    "    lon=longitude,\n",
    "    lat=latitude,\n",
    "    bands=measurements,\n",
    "    crs=output_crs,\n",
    "    resolution=resolution[1],\n",
    "    chunks=chunks,\n",
    ")\n",
    "\n",
    "# Squeeze to remove the defunct time dimension [otherwise we retain a default timestamp of 1970-01-01, which is not helpful].\n",
    "ds_astat = ds_astat.squeeze()\n",
    "\n",
    "# and as usual apply the CRS fix:\n",
    "ds_astat = fix_crs(ds_astat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at the summary of these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_astat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_astat.to_netcdf('ds_astat.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, figure out if we need to combine Landsat data with arealstatistik.\n",
    "\n",
    "if (ds_clean is not None) and (ds_astat is not None):\n",
    "    # In this case, you have loaded both Landsat and arealstatistik.\n",
    "    # So, let's combine them into a single Dataset, allowing them to be saved together.\n",
    "    ds_save = xr.merge([ds_clean, ds_astat])\n",
    "elif (ds_clean is not None):\n",
    "    # We are saving only the Landsat dataset\n",
    "    ds_save = ds_clean\n",
    "elif (ds_astat is not None):\n",
    "    # We are saving only the arealstatistik dataset\n",
    "    ds_save = ds_astat\n",
    "else:\n",
    "    raise ValueError('Hmm, unknown combination of data. Ask a teacher for help.')\n",
    "\n",
    "# you guessed correctly, we make sure to apply the fix (there is not always an issue, but this fix is very fast)\n",
    "ds_save = fix_crs(ds_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is what will be saved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file. Change the output filename to something useful!\n",
    "output_filename = 'mydata.nc'\n",
    "ds_save.to_netcdf(output_filename, engine=\"netcdf4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
