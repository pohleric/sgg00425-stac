{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbd3b0c-7837-4a6a-8648-73a64306825d",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:8px solid black\">\n",
    "<a name='part2'></a>\n",
    "\n",
    "# Trends, variability and deviation\n",
    "This notebook is split into three parts:\n",
    "\n",
    "1. <a href=\"#introduction\">Introduction</a> to the ideas behind trends, variability and deviation.\n",
    "2. Complete examples of <a href=\"#regression_ts\">regression for time series</a> without the spatial dimension.\n",
    "3. Complete example of temporal <a href=\"#regression_spts\">regression through every pixel</a> in a DataArray.\n",
    "\n",
    "***\n",
    "\n",
    "As usual we first need to load our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1faf9-5dae-4fd9-83c9-39525fa5874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# reload module before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# define modules locations (you might have to adapt define_mod_locs.py)\n",
    "# %run ../sdc-notebooks/Tools/define_mod_locs.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "from sdc_utilities import *\n",
    "\n",
    "# silence warning (not recommended during development)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b77b6-c3d3-49ae-aaa4-633e0b049473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especially for the beamer, we're going to use seaborn to make the figure text bigger.\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "plt.rcParams['figure.figsize'] = (16,8)       # this line changes the size of the figures displayed in the notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df81aa-f90b-4aa5-aa92-650ab7809d41",
   "metadata": {},
   "source": [
    "### IMPORTANT: run this cell to load the extra packages for regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f22fb8-b42d-4ee6-b386-78aaac5aa2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "from matplotlib.dates import date2num, num2date\n",
    "\n",
    "# two functions to convert back and forth\n",
    "def xr_date2num(time):\n",
    "    return date2num(time)\n",
    "\n",
    "def xr_num2date(time_numeric):\n",
    "    # transforms the num2date (days since ...) into datetime64 (seconds since ...)\n",
    "    return np.array([np.datetime64(d) for d in num2date(time_numeric)])\n",
    "\n",
    "# To check that the two functions work as intended:\n",
    "# # forward: date to numeric\n",
    "# xr_date2num(daa.time)\n",
    "# # backward:\n",
    "# xr_num2date(xr_date2num(daa.time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c51e04-299b-4317-971f-fc30194f942d",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:8px solid black\" />\n",
    "\n",
    "## Preparing/downloading our data\n",
    "\n",
    "We will use a pre-prepared small data subset around Fribourg which we extracted from the Swiss Data Cube for you earlier. <span class=\"dothis\">Download this dataset by running the next cell.</span> After a short while you should see the .nc file appear in the file explorer pane on your left (you may need to click the 'Refresh' button).\n",
    "\n",
    "<span style=\"color:gray; font-style:italic\">We made this data subset using `ts1_data_preparation.ipynb`. You will find this approach useful when doing your project work.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c2f66-fa01-412b-a3a1-74ffb0ae8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_filename = \"data/landsat_ot_c2_l2_fribourg_example.nc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c646a-f9ce-4212-86b2-acc1ba7d1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the prepared Landsat 8 subset for the Fribourg region \n",
    "ds = xr.open_dataset(nc_filename, engine='netcdf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89deab6e-0808-4e2e-8d5e-9769e6f047f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'shortcut' variable so that we can work with NDVI directly.\n",
    "ndvi = ds.ndvi\n",
    "ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72a66b-e4c2-446d-92b6-816b15d3c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To look at the trend through time in a small region, we'll use these data, resampled to monthly temporal frequency.\n",
    "point_x = 2580000\n",
    "point_y = 1181500\n",
    "ndvi_at_point = ndvi.sel(x=slice(point_x-1000, point_x+1000), y=slice(point_y+1000, point_y-1000)).mean(('x', 'y')).resample(time='MS').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cd16b-458f-4806-bc2c-9cef790362d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_at_point.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ea596-a16d-4fa7-bc93-f594fab89a68",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "<hr style=\"border-top:8px solid black\" />\n",
    "\n",
    "## Background: how to calculate trends and how to interpret them\n",
    "\n",
    "In our day-to-day life the words **trend** and **tendency** can often be used interchangeably; in the context of climate, however, they are different. \n",
    "\n",
    "> *Climate describes the average weather conditions for a particular location and over a long period of time.\n",
    "> $[...]$ climate normalsâ€”30-year historical averages of variables like temperature and precipitation [...]* (WMO 2022)\n",
    "\n",
    "The main use: **trend** is a **statistically significant change** over time in our variable. With other words, over a time period in a climatological context (>= 30 years), the values increase or decrease; and there is a very small chance that this is observed by chance. If there is less than **30 years** of data available, one can use the term **tendency** (this is not an agreed-upon term!!!) to highlight that this is not a climatological context.\n",
    "\n",
    "However, if we **de-trend** our data, it means that any systematic increase in values over time is removed. This independent of whether this change over time is **significant or non-significant**.\n",
    "\n",
    "***\n",
    "\n",
    "In order to be statistically significant the following must be fulfilled:\n",
    "- $p \\le \\alpha$\n",
    "\n",
    "The significance level $\\alpha$ is chosen by us (usually 5%). In the example of a trend, it is the probability of observing a change over time even though in reality that is not true. *The general definition: it is the probability of rejecting the Null-hypothesis $H_{0}$ (=no trend), given that the Null-hypothesis is true.*\n",
    ">With other words, we allow to make a mistake in 5% of the cases by assuming there is a trend even though there is no trend. Lowering the value of $\\alpha$ makes us more sure there is really a trend, but it also makes it more difficult to find one that is not as obvious.\n",
    "\n",
    "The $p-value$ (sometimes written only $p$) is the result of a statistical test. In the example of a trend, it is the probability of seeing a change over time as extreme as we do, assuming ($H_{0}$) there is no real trend. *The general definition: it is the probability obtaining a result as extreme, given that $H_{0}$ is true.*\n",
    "\n",
    "> Example: $H_{0} =$\"no trend\", $\\alpha=0.05$, $p-value=0.0231$ (outcome of our analysis). Because $p \\le \\alpha$, we reject $H_{0}$ and accept $H_{1}$ the alternative hypothesis that there is a trend. The result is statistically significant at our chosen significance level $\\alpha=0.05$. The lower the $p-value$, the less likely that an identified trend was identified even though in reality there is no trend. Reversing the wording: The lower the $p-value$, the more likely there is a real trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330de3e1-f83e-4e22-811e-25bf18404005",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### Doing the calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dbbe8-5127-46a1-bb5e-efd4d163e456",
   "metadata": {},
   "source": [
    "\n",
    "The `linregress()` function takes two arrays of values (x and y) to check if there is a relationship between them. `x` will be time and `y` the NDVI values. If there is a significant relationship between them, it means there is a significant change over time; with other words a statistically significant **trend** or **tendency**.\n",
    "\n",
    "The output of `linregress` function is multiple statistics. These are: \n",
    "\n",
    "#### Slope\n",
    "The **slope** says how much the change is **per time unit**. If we use monthly data, then the change units will be **change per month**. However, here we are using data with a daily frequency, so the change units will be **change per day**.\n",
    "\n",
    "#### Intercept\n",
    "The **intercept** is graphically the point on the y-axis where the regression line cuts through it at **x=0**. This statistic is only of interest for the graphical interpretation in our case.\n",
    "\n",
    "#### Correlation coefficient (r) and coefficient of determination (r$^{2}$)\n",
    "The **correlation coefficient** and the **coefficient of determination r$^{2}$** tell us how much of the variance is explained. With other words, how well our regression explains the relationship. \n",
    "\n",
    "#### p-value\n",
    "Statistically, maybe the most important outcome. **Is there really a change over time, or do we see something by chance?**. The lower, the more robust/striking.\n",
    "\n",
    "If the $r^{2}$ is high, you will always have a **low $p-value$**. But, you can have a low r$^{2}$ with a low $p-value$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5da5c-b3ce-47d0-ac3d-cacc49a02e59",
   "metadata": {},
   "source": [
    "### Removing non-valid data points `NaN`\n",
    "The `linregress` function is very strict with regards to missing data. We can only use data where there are no missing values (`NaN`). The next cell filters them away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d13b6-af83-45c1-bae9-044fafef2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our values for y and x\n",
    "y = ndvi_at_point.values  \n",
    "x = xr_date2num(ndvi_at_point.time.values)\n",
    "\n",
    "# this checks if the value is a valid numeric data point\n",
    "clean_mask = np.isfinite(y)  \n",
    "\n",
    "# the mask has the indices of valid data in y.\n",
    "# you can compare the before and after:\n",
    "# y\n",
    "# y[clean_mask]\n",
    "\n",
    "# The cleaning is applied to both:\n",
    "# - time\n",
    "# - ndvi\n",
    "# so that they have the same length\n",
    "y_clean = y[clean_mask]\n",
    "x_clean = x[clean_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867877a1-06a7-492b-8e65-b1c70deefa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally the regression\n",
    "result = linregress(x_clean, y_clean)\n",
    "print(result)\n",
    "\n",
    "# We are only interested in\n",
    "# - slope \n",
    "# - intercept (only graphical)\n",
    "# - p-value\n",
    "# - r-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fc52b-de07-4b31-9c4f-cf57080240ee",
   "metadata": {},
   "source": [
    "*** \n",
    "The output shows us that $p > \\alpha$. The change over time is thus not statistically significant. <span style=\"color:red\">The plots will in any case show you a line. **Do not fall into the trap to discuss the slope of the line *\"... there is a positive trend...\"* when it is not significant!**</span>\n",
    "\n",
    "The information on what the optimal (*ordinary least square regression*) regression looks like is stored in the `result` object. The `slope` and `intercept` can be assessed with `result.slope` and `result.intercept`; slope with `result.slope`, $p-value$ with `result.pvalue`, and $r$ with `result.rvalue`.\n",
    "\n",
    "The line can be created by plotting the x-values (time) against the values calculated from the simple formula for a line:\n",
    "\n",
    "$y = m*x + b$,\n",
    "\n",
    "where $m$ is the slope, $b$ is the intercept, $x$ are the time values (in days), and $y$ are the NDVI values.\n",
    "\n",
    "Using this information, let's plot the results of the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a257a-da7b-4246-bd01-d141b0a9fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the underlying data\n",
    "ndvi_at_point.plot.line(marker='o')\n",
    "\n",
    "# Plot the regression line\n",
    "m = result.slope\n",
    "b = result.intercept\n",
    "y_pred = m * x + b     # the predicted values; using all time steps (non-filtered)\n",
    "plt.plot(x, y_pred, 'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48aa49-43bf-4318-bdde-ca2d33563f23",
   "metadata": {},
   "source": [
    "### Slope units\n",
    "As mentioned before, we transform the time. If we look at the `slope` value, we can see a value of `-8.782572937241211e-07`. Since the transformed time values have the units of **days**, this value indicates a change of **-8.782572937241211e-07 units per day**.\n",
    "\n",
    "We can check quickly by looking at the predicted values and corresponding time entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d0b64-6805-4015-90e0-d93d9a5fdb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b6311-efec-4d8b-9fb1-07573e3f8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = x[0]\n",
    "t1 = x[1]\n",
    "\n",
    "dt = t1 - t0\n",
    "print('The difference in days between the two time steps:',dt)\n",
    "\n",
    "# the predicted NDVI values\n",
    "ndvi_predicted = m * x + b\n",
    "ndvi_pred0 = ndvi_predicted[0]\n",
    "ndvi_pred1 = ndvi_predicted[1]\n",
    "\n",
    "dndvi = ndvi_pred1 - ndvi_pred0\n",
    "print('The difference in NDVI from the regression between the two time steps:',dndvi)\n",
    "\n",
    "rate = dndvi/dt\n",
    "print('The slope is:',rate)\n",
    "\n",
    "# Quick check if this rate is the same as from the regression (ratio=1):\n",
    "print(\"This should be close to 1:\",rate/result.slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f90df-6b17-434f-95a5-0efcf8ea3daa",
   "metadata": {},
   "source": [
    "<a name=\"regression_ts\"></a>\n",
    "<hr style=\"border-top:8px solid black\" />\n",
    "\n",
    "## Complete examples for time series data\n",
    "\n",
    "This section shows you how to do regression on DataArrays **with** a `time` dimension but **without** `x` and `y` dimensions.\n",
    "\n",
    "In the following you have two examples with a full workflow:\n",
    "- selecting data\n",
    "- averaging the data\n",
    "- resampling to monthly time steps\n",
    "- filtering NaN values\n",
    "- running linregress\n",
    "- plotting\n",
    "\n",
    "Both examples use the function in the cell below which we have written for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98711f52-f857-4069-a8f0-1de827248eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_regression(input_data, plot_results=True):\n",
    "    # regression preparation:\n",
    "    y = input_data.values  \n",
    "    x = xr_date2num(input_data.time.values)  \n",
    "\n",
    "    # only take non-NaN values\n",
    "    clean_mask = np.isfinite(y)  \n",
    "\n",
    "    y_clean = y[clean_mask]\n",
    "    x_clean = x[clean_mask]\n",
    "\n",
    "    # regression\n",
    "    reg = linregress(x_clean, y_clean)\n",
    "    \n",
    "    if plot_results:\n",
    "\n",
    "        # slope\n",
    "        m = reg.slope\n",
    "        # intercept\n",
    "        b = reg.intercept\n",
    "\n",
    "        # calculate regression line (uncleaned data)\n",
    "        y_pred = reg.slope * x + reg.intercept\n",
    "        # calculate regression line (cleaned data)\n",
    "        y_pred_mon = reg.slope * x_clean + reg.intercept\n",
    "\n",
    "        # plot\n",
    "        da_mon.plot.line(marker='o', linestyle='-', color='tab:blue')\n",
    "        plt.plot(x_clean, y_pred_mon, linestyle='-', color='tab:red')\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da121c1a-144c-4336-8df1-d1dfab72ef61",
   "metadata": {},
   "source": [
    "#### Regression over September data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eeea97-b31a-44b3-8b8e-9b61a6f88e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our spatial subset\n",
    "da = ndvi.sel(x=slice(point_x-5000, point_x-1000), y=slice(point_y+6000, point_y+2000))\n",
    "# select all values from September\n",
    "da_mon = da.sel(time=da.time.dt.month == 9)\n",
    "# resample to monthly mean at each pixel\n",
    "da_mon = da_mon.resample(time='MS').mean()\n",
    "# average over all pixels per time step\n",
    "da_mon = da_mon.mean(('x', 'y'))\n",
    "\n",
    "\n",
    "# Do the regression. By default this function also produces a plot for you.\n",
    "reg = point_regression(da_mon)\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078d39e-ec37-4e36-83d1-4778d01f39b1",
   "metadata": {},
   "source": [
    "#### Regression over a single year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400628d-e006-4a90-9eb7-5e8533feaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial subset\n",
    "da = ndvi.sel(x=slice(point_x-5000, point_x-1000), y=slice(point_y+6000, point_y+2000))\n",
    "# Let's look at just a year\n",
    "da_mon = da.sel(time='2015')\n",
    "# resample to monthly means\n",
    "da_mon = da_mon.resample(time='MS').mean()\n",
    "# average over all latitudes and longitudes per time step\n",
    "da_mon = da_mon.mean(('x', 'y'))\n",
    "# Do the regression\n",
    "reg = point_regression(da_mon)\n",
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e91c5-d4a2-4498-98d4-293fa906b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To just get the regression results without a plot:\n",
    "reg = point_regression(da_mon, plot_results=False)\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447d560-9346-4d4f-befa-b636c68e209d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"regression_spts\"></a>\n",
    "<hr style=\"border-top:8px solid black\" />\n",
    "\n",
    "## Complete example for spatio-temporal trends\n",
    "It is possible to calculate the trend through time for every pixel in our datacube. This allows to compare pixel by pixel in the form of a map.\n",
    "\n",
    "We've written a function for you that cleans your DataArray and then calculates the trend in every pixel. It's contained in the cell below, you don't need to change anything in this function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e3bca-74dc-446d-865b-44eb23f20a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatio_temporal_trend(dataset):\n",
    "\n",
    "    # Don't change anything in the code below.\n",
    "    x = np.arange(dataset.time.shape[0])\n",
    "\n",
    "    def new_linregress(y):\n",
    "        # Wrapper around scipy linregress to use in apply_ufunc\n",
    "        clean_mask = np.isfinite(y)  \n",
    "        y_clean = y[clean_mask]\n",
    "        x_clean = x[clean_mask]\n",
    "        slope, intercept, r_value, p_value, _ = linregress(x_clean, y_clean)\n",
    "        return np.array([slope, intercept, r_value, p_value])\n",
    "\n",
    "    stats = xr.apply_ufunc(new_linregress, dataset, \n",
    "                           input_core_dims=[['time']],\n",
    "                           output_core_dims=[[\"parameter\"]],\n",
    "                           vectorize=True,\n",
    "                           dask=\"parallelized\",\n",
    "                           output_dtypes=['float64'],\n",
    "                           output_sizes={\"parameter\": 4},\n",
    "                          )\n",
    "\n",
    "    # Give the stats parameters their proper names\n",
    "    stats['parameter'] = ['slope', 'intercept', 'r_value', 'p_value']\n",
    "    # Convert to an xarray Dataset\n",
    "    stats = stats.to_dataset(dim='parameter')\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e593ab-5fe9-4bf5-9c88-5fb562eea6e3",
   "metadata": {},
   "source": [
    "To use this function, simply supply it with a DataArray with time, x and y coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452709ce-4a72-4bb7-b6e2-05197bc5f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = spatio_temporal_trend(ndvi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ac011-a33c-47e9-84b9-4823a4cf0b97",
   "metadata": {},
   "source": [
    "This gives us an xarray Dataset with four data variables. We get information on the slope, intercept, r-value and p-value returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4f9ab-6c0a-4b5a-b940-6459d597157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd0943-8640-4034-b1ab-a70ef9df7409",
   "metadata": {},
   "source": [
    "Let's have a look at the r-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb4ade-aff2-4323-a792-20ddd795659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.r_value.plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478624a-cbaf-4556-b067-e279226f8ca9",
   "metadata": {},
   "source": [
    "We can also take a look at the p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efd133-35ab-4f67-8a93-8691e3996490",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.p_value.plot.imshow(cmap='viridis_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb84a41-113a-4f47-b5d3-7a06f11a71af",
   "metadata": {},
   "source": [
    "Finally, if we think back to the glacier mapping practical, remember that we can do a few things to help visualise these data:\n",
    "\n",
    "1) We can set a threshold to show only pixels that meet certain criteria;\n",
    "2) We can use transparency to overlay two images.\n",
    "\n",
    "Let's take a look at these now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88ff4a-2c5c-429e-90af-55dc1fd8108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a map which only shows the r-value at pixels where $p$ is less than 0.05.\n",
    "\n",
    "# First, we'll put a picture of NDVI in the background.\n",
    "ndvi.mean(dim='time').plot(cmap='Greys_r', vmin=0, vmax=1)\n",
    "\n",
    "# On top of this we'll draw only the pixles meeting our p-value criteria of 5% probability.\n",
    "stats.r_value.where(stats.p_value <= 0.05).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
